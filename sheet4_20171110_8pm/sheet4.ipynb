{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group: LGLLK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sheet 4: Rounding, Overflow, Linear Algebra\n",
    "\n",
    "In this exercise sheet, we look at various sources of numerical overflow when executing Python and numpy code for large input values, and how to efficiently handle them, for example, by using numpy special functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy,utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a robust \"softplus\" nonlinear function (40 P)\n",
    "\n",
    "The softplus function is defined as:\n",
    "\n",
    "$$\n",
    "\\mathrm{softplus}(x) = \\log(1+\\exp(x)).\n",
    "$$\n",
    "\n",
    "It intervenes as elementary computation in certain machine learning models such as neural networks. Plotting it gives the following curve\n",
    "\n",
    "![plot generated using fooplot.com](softplus.png)\n",
    "\n",
    "where the function tends to zero for very negative input values and tends to the identity for very positive input values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softplus(z): return numpy.log(1+numpy.exp(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider an input vector from the module `utils` containing varying values between 1 and 10000. We would like to apply the `softplus` function to all of its element in an element-wise manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-10000, -1000, -100, -10, -1, 0, 1, 10, 100, 1000, 10000]\n"
     ]
    }
   ],
   "source": [
    "X = utils.softplus_inputs\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose these large values in order to test whether the behavior of the function is correct in all regimes of the function, in particular, for very small or very large values. The code below applies the `softplus` function directly to the vector of inputs and then prints for all cases the input and the corresponding function output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softplus(-10000.0000) =      0.0000\n",
      "softplus( -1000.0000) =      0.0000\n",
      "softplus(  -100.0000) =      0.0000\n",
      "softplus(   -10.0000) =      0.0000\n",
      "softplus(    -1.0000) =      0.3133\n",
      "softplus(     0.0000) =      0.6931\n",
      "softplus(     1.0000) =      1.3133\n",
      "softplus(    10.0000) =     10.0000\n",
      "softplus(   100.0000) =    100.0000\n",
      "softplus(  1000.0000) =         inf\n",
      "softplus( 10000.0000) =         inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:1: RuntimeWarning: overflow encountered in exp\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "Y = softplus(X)\n",
    "for x,y in zip(X,Y):\n",
    "    print('softplus(%11.4f) = %11.4f'%(x,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For large input values, the softplus function returns `inf` whereas analysis of that function tells us that it should compute the identity. Let's now try to apply the softplus function one element at a time, to see whether the problem comes from numpy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softplus(-10000.0000) =      0.0000\n",
      "softplus( -1000.0000) =      0.0000\n",
      "softplus(  -100.0000) =      0.0000\n",
      "softplus(   -10.0000) =      0.0000\n",
      "softplus(    -1.0000) =      0.3133\n",
      "softplus(     0.0000) =      0.6931\n",
      "softplus(     1.0000) =      1.3133\n",
      "softplus(    10.0000) =     10.0000\n",
      "softplus(   100.0000) =    100.0000\n",
      "softplus(  1000.0000) =         inf\n",
      "softplus( 10000.0000) =         inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:1: RuntimeWarning: overflow encountered in exp\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "for x in X:\n",
    "    y = softplus(x)\n",
    "    print('softplus(%11.4f) = %11.4f'%(x,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the result is the same. We observe that the function always stops working when its output approaches 1000, even though the input was given in high precision `float64`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create an alternative function for `softplus` that applies to input scalars and that correctly applies to values that can be much larger than 1000 (e.g. billions or more). Your function can be written in Python directly and does not need numpy parallelization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softplus_opt(Z):\n",
    "    result = []\n",
    "    for z in Z:\n",
    "        if z > 100:\n",
    "            result += [z]\n",
    "        else:\n",
    "            result += [numpy.log(1+numpy.exp(z))]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softplus(-10000.0000) =      0.0000\n",
      "softplus( -1000.0000) =      0.0000\n",
      "softplus(  -100.0000) =      0.0000\n",
      "softplus(   -10.0000) =      0.0000\n",
      "softplus(    -1.0000) =      0.3133\n",
      "softplus(     0.0000) =      0.6931\n",
      "softplus(     1.0000) =      1.3133\n",
      "softplus(    10.0000) =     10.0000\n",
      "softplus(   100.0000) =    100.0000\n",
      "softplus(  1000.0000) =   1000.0000\n",
      "softplus( 10000.0000) =  10000.0000\n"
     ]
    }
   ],
   "source": [
    "Y = softplus_opt(X)\n",
    "for x,y in zip(X,Y):\n",
    "    print('softplus(%11.4f) = %11.4f'%(x,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen in the previous exercise sheet, the problem of functions that apply to scalars only is that they are less efficient than functions that apply to vectors directly. Therefore, we would like to handle the rounding issue directly at the vector level.\n",
    "\n",
    "* Create a new softplus function that applies to vectors and that has the desired behavior for large input values. Your function should be fast for large input vectors (i.e. it is not appropriate to use an inner Python loop inside the function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softplus3(-10000.0000) =      0.0000\n",
      "softplus3( -1000.0000) =      0.0000\n",
      "softplus3(  -100.0000) =      0.0000\n",
      "softplus3(   -10.0000) =      0.0000\n",
      "softplus3(    -1.0000) =      0.3133\n",
      "softplus3(     0.0000) =      0.6931\n",
      "softplus3(     1.0000) =      1.3133\n",
      "softplus3(    10.0000) =     10.0000\n",
      "softplus3(   100.0000) =    100.0000\n",
      "softplus3(  1000.0000) =   1000.0000\n",
      "softplus3( 10000.0000) =  10000.0000\n"
     ]
    }
   ],
   "source": [
    "# using clip to give the boundary of input vector. \n",
    "# Ref in lecture4 \"The sigmoid function (4)\"\n",
    "\n",
    "def softplus_npopt(Z):\n",
    "    clipZ = numpy.clip(Z, -10000, 100)\n",
    "    Y = numpy.log(1+numpy.exp(clipZ))\n",
    "    results = numpy.where(clipZ == 100, Z, Y)\n",
    "    return results\n",
    "    \n",
    "Y = softplus_npopt(X)\n",
    "\n",
    "for x,y in zip(X,Y):\n",
    "    print('softplus3(%11.4f) = %11.4f'%(x,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing a partition function (30 P)\n",
    "\n",
    "We consider a discrete probability distribution of type\n",
    "$$\n",
    "p(\\boldsymbol{x};\\boldsymbol{w}) = \\frac{1}{Z(\\boldsymbol{w})} \\exp(\\boldsymbol{x}^\\top \\boldsymbol{w})\n",
    "$$\n",
    "where $\\boldsymbol{x} \\in \\{-1,1\\}^{10}$ is an observation, and $\\boldsymbol{w} \\in \\mathbb{R}^{10}$ is a vector of parameters. The term $Z(\\boldsymbol{w})$ is called the partition function and is chosen such that the probability distribution sums to 1. That is, the equation:\n",
    "$$\n",
    "\\sum_{\\boldsymbol{x} \\in \\{-1,1\\}^{10}} p(\\boldsymbol{x};\\boldsymbol{w}) = 1\n",
    "$$\n",
    "must be satisfied. Below is a simple method that computes the log of the partition function $Z(\\boldsymbol{w})$ for various choices of parameter vectors. The considered parameters (`w_small`, `w_medium`, and `w_large`) are increasingly large (and thus problematic), and can be found in the file `utils.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    18.2457\n",
      "    89.5932\n",
      "        inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:7: RuntimeWarning: overflow encountered in exp\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "import numpy,utils\n",
    "import itertools\n",
    "\n",
    "def getlogZ(w):\n",
    "    Z = 0\n",
    "    for x in itertools.product([-1, 1], repeat=10):\n",
    "        Z += numpy.exp(numpy.dot(x,w))\n",
    "    return numpy.log(Z)\n",
    "\n",
    "print('%11.4f'%getlogZ(utils.w_small))\n",
    "print('%11.4f'%getlogZ(utils.w_medium))\n",
    "print('%11.4f'%getlogZ(utils.w_big))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe from these results, that for parameter vectors with large values (e.g. `utils.w_big`), the exponential function overflows, and thus, we do not obtain a correct value for the logarithm of `Z`.\n",
    "\n",
    "* Implement an improved function that avoids the overflow problem, and evaluate the partition function for the same parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    18.2457\n",
      "    89.5932\n",
      " 24921.9913\n"
     ]
    }
   ],
   "source": [
    "def getlogZ_opt(w):\n",
    "    \n",
    "    # use the log-sum-exp trick. for the proof see \n",
    "    # https://www.xarg.org/2016/06/the-log-sum-exp-trick-in-machine-learning/    \n",
    "    \n",
    "    argForExp_i1 = []\n",
    "    \n",
    "    for x in itertools.product([-1, 1], repeat=10):\n",
    "        argForExp = numpy.dot(x,w)\n",
    "        argForExp_i1.append( argForExp )\n",
    "    \n",
    "    offsetForExpArgs = max( argForExp_i1 )\n",
    "     \n",
    "    Zrest = 0.\n",
    "    for x in itertools.product([-1, 1], repeat=10):\n",
    "        Zrest += numpy.exp ( numpy.dot(x,w)-offsetForExpArgs )\n",
    "    \n",
    "    lnZ = offsetForExpArgs + numpy.log(Zrest)\n",
    "    \n",
    "    return lnZ\n",
    "\n",
    "print('%11.4f'%getlogZ_opt(utils.w_small))\n",
    "print('%11.4f'%getlogZ_opt(utils.w_medium))\n",
    "print('%11.4f'%getlogZ_opt(utils.w_big))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For the model with parameter `utils.w_big`, evaluate the log-probability of the binary vectors contained in the list `itertools.product([-1, 1], repeat=10)`, and return the indices (starting from 0) of those that have probability greater or equal to 0.001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[81, 83, 85, 87, 209, 211, 213, 215, 337, 339, 341, 343, 465, 467, 469, 471, 597, 599, 725, 727, 853, 855, 981, 983]\n"
     ]
    }
   ],
   "source": [
    "lnZ_wBig = getlogZ_opt( utils.w_big )\n",
    "    \n",
    "def logP(x):\n",
    "    logP = - lnZ_wBig + numpy.dot(x,utils.w_big) \n",
    "    return logP\n",
    "\n",
    "def getIndicesOfProbsLargerOrEqualThan( probLowerBound ):        \n",
    "    iBinVec = 0\n",
    "    iLargerOrEqual = []\n",
    "    for binVec in itertools.product([-1, 1], repeat=10):\n",
    "        logPBinVec = logP(binVec)\n",
    "        pBinVec = numpy.exp( logPBinVec )\n",
    "        if( pBinVec >= probLowerBound ):\n",
    "            iLargerOrEqual.append( iBinVec )\n",
    "        iBinVec += 1\n",
    "\n",
    "    return iLargerOrEqual\n",
    "\n",
    "print( getIndicesOfProbsLargerOrEqualThan( 1e-3 ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability of generating data from a Gaussian model (30 P)\n",
    "\n",
    "Consider a multivariate Gaussian distribution of mean vector `m` and covariance `S`. The probability associated to a vector `x` is given by:\n",
    "\n",
    "$$\n",
    "p(\\boldsymbol{x};(\\boldsymbol{m},S)) = \\frac{1}{\\sqrt{(2\\pi)^d \\mathrm{det}(S)}} \\exp \\Big( - \\frac12 (\\boldsymbol{x}-\\boldsymbol{m})^\\top S^{-1} (\\boldsymbol{x}-\\boldsymbol{m})\\Big)\n",
    "$$\n",
    "\n",
    "We consider the calculation of the probability of observing a certain dataset \n",
    "\n",
    "$$\n",
    "\\mathcal{D} = (\\boldsymbol{x}^{(1)},\\dots,\\boldsymbol{x}^{(N)})\n",
    "$$\n",
    "\n",
    "assuming the data is generated according to a Gaussian distribution of fixed parameters $\\boldsymbol{m}$ and $S$. Such probability density is given by the formula:\n",
    "\n",
    "$$\n",
    "\\log P(\\mathcal{D};(\\boldsymbol{m},S)) = \\log \\prod_{i=1}^N p(\\boldsymbol{x}^{(i)};(\\boldsymbol{m},S))\n",
    "$$\n",
    "\n",
    "The function below implements such function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy,numpy.linalg,utils\n",
    "\n",
    "def logp(X,m,S):\n",
    "    \n",
    "    # Find the number of dimensions from the data vector\n",
    "    d = X.shape[1]\n",
    "    \n",
    "    # Invert the covariance matrix\n",
    "    Sinv = numpy.linalg.inv(S)\n",
    "    \n",
    "    # Compute the quadratic terms for all data points\n",
    "    Q = -0.5*(numpy.dot(X-m,Sinv)*(X-m)).sum(axis=1)\n",
    "    \n",
    "    # Raise them quadratic terms to the exponential\n",
    "    Q = numpy.exp(Q)\n",
    "    \n",
    "    # Divide by the terms in the denominator\n",
    "    P = Q / numpy.sqrt((2*numpy.pi)**d * numpy.linalg.det(S))\n",
    "    \n",
    "    # Take the product of the probability of each data points\n",
    "    Pprod = numpy.prod(P)\n",
    "    \n",
    "    # Return the log-probability\n",
    "    return numpy.log(Pprod)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of this function for various datasets and parameters provided in the file `utils.py` gives the following probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:24: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-13.0067700574\n",
      "-inf\n",
      "-inf\n"
     ]
    }
   ],
   "source": [
    "print(logp(utils.X1,utils.m1,utils.S1))\n",
    "print(logp(utils.X2,utils.m2,utils.S2))\n",
    "print(logp(utils.X3,utils.m3,utils.S3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is numerically instable for multiple reasons. The product of many probabilities, the inversion of a large covariance matrix, and the computation of its determinant, are all potential causes for overflow. Thus, we would like to find a numerically robust way of performing each of these.\n",
    "\n",
    "* Implement a numerically stable version of the function `logp`\n",
    "* Evaluate it on the same datasets and parameters as the function `logp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-13.0067700574\n",
      "-1947.97098067\n",
      "-inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:36: RuntimeWarning: divide by zero encountered in log\n"
     ]
    }
   ],
   "source": [
    "#This function is numerically instable for multiple reasons. \n",
    "# 1 The product of many probabilities, \n",
    "# 2 the inversion of a large covariance matrix, \n",
    "# 3 and the computation of its determinant, \n",
    "# are all potential causes for overflow. \n",
    "\n",
    "def logp_opt(X,m,S):\n",
    "    \n",
    "    # Find the number of dimensions from the data vector\n",
    "    d = X.shape[1]\n",
    "    \n",
    "    # prob2 = inverting large matrix. \n",
    "    L = numpy.linalg.cholesky(S)\n",
    "    LT = numpy.transpose( L )\n",
    "    L_inv = numpy.linalg.inv(L)\n",
    "    LT_inv = numpy.linalg.inv(LT)\n",
    "    # prob3 = computation of det.    \n",
    "    detS = numpy.linalg.det( L ) * numpy.linalg.det( LT )\n",
    "    \n",
    "    # Compute the quadratic terms for all data points\n",
    "    vecDev = X-m\n",
    "    vecDevMapped1 = numpy.dot( vecDev , LT_inv )\n",
    "    vecDevMapped2 = numpy.dot( vecDevMapped1 , L_inv )\n",
    "    prodComponents_i1 = vecDevMapped2 * vecDev\n",
    "    scalProd = prodComponents_i1.sum(axis=1)\n",
    "    Q = -0.5*scalProd\n",
    "    \n",
    "    # Raise them quadratic terms to the exponential\n",
    "    Q = numpy.exp(Q)\n",
    "\n",
    "    # Divide by the terms in the denominator\n",
    "    P = Q / numpy.sqrt((2*numpy.pi)**d * detS )\n",
    "    \n",
    "    # prob1 -> solution = replace prod by sum.\n",
    "    # Take the product of the probability of each data points\n",
    "    lnP = numpy.sum( numpy.log(P) )\n",
    "    \n",
    "    # Return the log-probability\n",
    "    return lnP\n",
    "\n",
    "print(logp_opt(utils.X1,utils.m1,utils.S1))\n",
    "print(logp_opt(utils.X2,utils.m2,utils.S2))\n",
    "print(logp_opt(utils.X3,utils.m3,utils.S3))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
